{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "XOR BACKPROPAGATION.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNS0wqIadYbL2kS3VX5l0dC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pko89403/DeepLearningSelfStudy/blob/master/XOR_BACKPROPAGATION.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pm3PAlY5SgFJ",
        "colab_type": "text"
      },
      "source": [
        "인공신경망...Neuron\n",
        "Same Shit And ~ OR ~ XOR ORIGIN OF PERCEPTRON"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-iIsGZuU0q2",
        "colab_type": "text"
      },
      "source": [
        "### Backpropagation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SMrYLal2Sdtb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device('cuda')\n",
        "torch.cuda.manual_seed_all(777)\n",
        "\n",
        "X = torch.FloatTensor([[0, 0], [0, 1], [1, 0], [1, 1]]).to(device)\n",
        "Y = torch.FloatTensor([[0], [1], [1], [0]]).to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fn3gYu4IVTJO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# nn Layers \n",
        "\n",
        "w1 = torch.Tensor(2, 2).to(device)\n",
        "b1 = torch.Tensor(2).to(device)\n",
        "w2 = torch.Tensor(2, 1).to(device)\n",
        "b2 = torch.Tensor(1).to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QRmStT3UV2Hu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sigmoid(x):\n",
        "  # sigmoid function\n",
        "  return 1.0 / (1.0 + torch.exp(-x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fOCymH2MV7aO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sigmoid_prime(x):\n",
        "  # 시그모이드 함수 미분\n",
        "  return sigmoid(x) * ( 1 - sigmoid(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sf0nG8sHGJKH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        },
        "outputId": "64db72e7-0782-4c4a-94a2-34a84eecd12b"
      },
      "source": [
        "learning_rate = 0.001\n",
        "\n",
        "for step in range(100):\n",
        "  l1 = torch.add(torch.matmul(X, w1), b1)\n",
        "  a1 = sigmoid(l1)\n",
        "\n",
        "  l2 = torch.add(torch.matmul(a1, w2), b2)\n",
        "  Y_pred = sigmoid(l2)\n",
        "\n",
        "  cost = -torch.mean(Y * torch.log(Y_pred) + (1-Y)*torch.log(1-Y_pred))\n",
        "\n",
        "  # backpropagation\n",
        "  # Loss derivative\n",
        "  d_Y_pred = (Y_pred - Y) / (Y_pred * (1.0 - Y_pred) + 1e-7)\n",
        "\n",
        "  # layer2\n",
        "  d_l2 = d_Y_pred * sigmoid_prime(l2)\n",
        "  d_b2 = d_l2\n",
        "  d_w2 = torch.matmul(torch.transpose(a1, 0, 1), d_b2)\n",
        "  # layer1\n",
        "  d_a1 = torch.matmul(d_b2, torch.transpose(w2, 0, 1))\n",
        "  d_l1 = d_a1 * sigmoid_prime(l1)\n",
        "  d_b1 = d_l1\n",
        "  d_w1 = torch.matmul(torch.transpose(X, 0, 1), d_b1)\n",
        "  # weight update\n",
        "  w1 = w1 - learning_rate * d_w1\n",
        "  b1 = b1 - learning_rate * torch.mean(d_b1, 0)\n",
        "  w2 = w2 - learning_rate * d_w2\n",
        "  b2 = b2 - learning_rate * torch.mean(d_b2, 0)\n",
        "\n",
        "  if step % 10 == 0:\n",
        "    print(step, cost.item())\n",
        "\n"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 nan\n",
            "10 nan\n",
            "20 nan\n",
            "30 nan\n",
            "40 nan\n",
            "50 nan\n",
            "60 nan\n",
            "70 nan\n",
            "80 nan\n",
            "90 nan\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "22TzB6hwH9Q7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}