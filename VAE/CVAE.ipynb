{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init\n",
    "import torchvision.datasets as dataset\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "learning_rate = 0.0005\n",
    "num_epoch = 20\n",
    "hidden_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data_CVAE/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.1%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data_CVAE/MNIST/raw/train-images-idx3-ubyte.gz to ./data_CVAE/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data_CVAE/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "113.5%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data_CVAE/MNIST/raw/train-labels-idx1-ubyte.gz to ./data_CVAE/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data_CVAE/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.4%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data_CVAE/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data_CVAE/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data_CVAE/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "180.4%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data_CVAE/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data_CVAE/MNIST/raw\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "mnist_train = dataset.MNIST(\"./data_CVAE\", train=True, transform=transforms.ToTensor(), target_transform=None, download=True)\n",
    "mnist_test = dataset.MNIST(\"./data_CVAE\", train=False, transform=transforms.ToTensor(), target_transform=None, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils. data.DataLoader(mnist_train, batch_size=batch_size, shuffle=True, num_workers=2, drop_last=True)\n",
    "test_loader = torch.utils. data.DataLoader(mnist_test, batch_size=batch_size, shuffle=True, num_workers=2, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.fc1 = nn.Sequential(\n",
    "                        nn.Conv2d(1, 8, 3, padding=1),# batch x 8 * 28 * 28\n",
    "                        nn.BatchNorm2d(8),\n",
    "                        nn.ReLU(),\n",
    "                        nn.MaxPool2d(2,2),\n",
    "                        nn.Conv2d(8, 16, 3, padding=1),#batch x 16 * 14 * 14\n",
    "                        nn.BatchNorm2d(16),\n",
    "                        nn.ReLU(),\n",
    "                        nn.MaxPool2d(2,2),\n",
    "                        nn.Conv2d(16, 32, 3, padding=1),#batch x 32 * 7 * 7\n",
    "                        nn.ReLU(),\n",
    "        )\n",
    "        self.fc2_1 = nn.Sequential(\n",
    "                            nn.Linear(32*7*7, 800),\n",
    "                            nn.Linear(800, hidden_size),\n",
    "        )\n",
    "        self.fc2_2 = nn.Sequential(\n",
    "                            nn.Linear(32*7*7, 800),\n",
    "                            nn.Linear(800, hidden_size),\n",
    "        )\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def encode(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = out.view(batch_size, -1)\n",
    "        out = self.relu(out)\n",
    "        mu = self.fc2_1(out)\n",
    "        log_var = self.fc2_2(out)\n",
    "        \n",
    "        return mu, log_var\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = logvar.mul(0.5).exp_()\n",
    "        eps = torch.FloatTensor(std.size()).normal_()\n",
    "        return eps.mul(std).add_(mu)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        reparam = self.reparameterize(mu, logvar)\n",
    "        return mu, logvar, reparam\n",
    "    \n",
    "encoder = Encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.fc1 = nn.Sequential(\n",
    "                        nn.Linear(hidden_size, 800),\n",
    "                        nn.BatchNorm1d(800),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Linear(800, 1568),\n",
    "                        nn.ReLU(),\n",
    "        )\n",
    "        self.fc2 = nn.Sequential(\n",
    "                        nn.ConvTranspose2d(32, 16, 3, 2, 1, 1),\n",
    "                        nn.ReLU(),\n",
    "                        nn.BatchNorm2d(16),\n",
    "                        nn.ConvTranspose2d(16, 8, 3, 2, 1, 1),\n",
    "                        nn.ReLU(),\n",
    "                        nn.BatchNorm2d(8),\n",
    "                        nn.ConvTranspose2d(8, 1, 3, 1, 1),\n",
    "                        nn.BatchNorm2d(1),\n",
    "        )\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = out.view(batch_size, 32, 7, 7)\n",
    "        out = self.fc2(out)\n",
    "        out = self.sigmoid(out)\n",
    "        out = out.view(batch_size, 28, 28, 1)\n",
    "        \n",
    "        return out \n",
    "    \n",
    "decoder = Decoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amore/opt/anaconda3/envs/Pytorch/lib/python3.7/site-packages/torch/nn/_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "reconstruction_function = nn.BCELoss(size_average=False)\n",
    "\n",
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    BCE = reconstruction_function(recon_x, x)\n",
    "    \n",
    "    KLD_element = mu.pow(2).add_(logvar.exp()).mul_(-1).add_(1).add_(logvar)\n",
    "    KLD = torch.sum(KLD_element).mul_(-0.5)\n",
    "    \n",
    "    return BCE + KLD\n",
    "\n",
    "parameters = list(encoder.parameters()) + list(decoder.parameters())\n",
    "optimizer = torch.optim.Adam(parameters, lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------model not restored----------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amore/opt/anaconda3/envs/Pytorch/lib/python3.7/site-packages/torch/nn/modules/loss.py:498: UserWarning: Using a target size (torch.Size([128, 1, 28, 28])) that is different to the input size (torch.Size([128, 28, 28, 1])) is deprecated. Please ensure they have the same size.\n",
      "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n",
      "/Users/amore/opt/anaconda3/envs/Pytorch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Encoder. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/Users/amore/opt/anaconda3/envs/Pytorch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Sequential. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/Users/amore/opt/anaconda3/envs/Pytorch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Conv2d. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/Users/amore/opt/anaconda3/envs/Pytorch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type BatchNorm2d. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/Users/amore/opt/anaconda3/envs/Pytorch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ReLU. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/Users/amore/opt/anaconda3/envs/Pytorch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type MaxPool2d. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/Users/amore/opt/anaconda3/envs/Pytorch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/Users/amore/opt/anaconda3/envs/Pytorch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Decoder. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/Users/amore/opt/anaconda3/envs/Pytorch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type BatchNorm1d. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/Users/amore/opt/anaconda3/envs/Pytorch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ConvTranspose2d. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/Users/amore/opt/anaconda3/envs/Pytorch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Sigmoid. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(80250.8359, grad_fn=<AddBackward0>)\n",
      "tensor(66831.0312, grad_fn=<AddBackward0>)\n",
      "tensor(63816.9102, grad_fn=<AddBackward0>)\n",
      "tensor(62233.7266, grad_fn=<AddBackward0>)\n",
      "tensor(61621.0352, grad_fn=<AddBackward0>)\n",
      "tensor(60327.5391, grad_fn=<AddBackward0>)\n",
      "tensor(59597.8203, grad_fn=<AddBackward0>)\n",
      "tensor(58976.2656, grad_fn=<AddBackward0>)\n",
      "tensor(58019.7656, grad_fn=<AddBackward0>)\n",
      "tensor(57510.2344, grad_fn=<AddBackward0>)\n",
      "tensor(57318.5469, grad_fn=<AddBackward0>)\n",
      "tensor(56948.6055, grad_fn=<AddBackward0>)\n",
      "tensor(55920.7812, grad_fn=<AddBackward0>)\n",
      "tensor(56729.0078, grad_fn=<AddBackward0>)\n",
      "tensor(54851.8984, grad_fn=<AddBackward0>)\n",
      "tensor(55863.8047, grad_fn=<AddBackward0>)\n",
      "tensor(55040.3242, grad_fn=<AddBackward0>)\n",
      "tensor(54555.1016, grad_fn=<AddBackward0>)\n",
      "tensor(54736.7656, grad_fn=<AddBackward0>)\n",
      "tensor(53938.2031, grad_fn=<AddBackward0>)\n",
      "tensor(53846.7852, grad_fn=<AddBackward0>)\n",
      "tensor(53883.1328, grad_fn=<AddBackward0>)\n",
      "tensor(53090.6172, grad_fn=<AddBackward0>)\n",
      "tensor(53331.6328, grad_fn=<AddBackward0>)\n",
      "tensor(53531.7734, grad_fn=<AddBackward0>)\n",
      "tensor(53132.3086, grad_fn=<AddBackward0>)\n",
      "tensor(52408.9297, grad_fn=<AddBackward0>)\n",
      "tensor(51717.3086, grad_fn=<AddBackward0>)\n",
      "tensor(52157.5664, grad_fn=<AddBackward0>)\n",
      "tensor(50866.1328, grad_fn=<AddBackward0>)\n",
      "tensor(51765.3125, grad_fn=<AddBackward0>)\n",
      "tensor(51285.4492, grad_fn=<AddBackward0>)\n",
      "tensor(50140.1484, grad_fn=<AddBackward0>)\n",
      "tensor(50428.4219, grad_fn=<AddBackward0>)\n",
      "tensor(50068.0039, grad_fn=<AddBackward0>)\n",
      "tensor(49726.1484, grad_fn=<AddBackward0>)\n",
      "tensor(49571.0703, grad_fn=<AddBackward0>)\n",
      "tensor(50121.2539, grad_fn=<AddBackward0>)\n",
      "tensor(49348.8906, grad_fn=<AddBackward0>)\n",
      "tensor(49150.2227, grad_fn=<AddBackward0>)\n",
      "tensor(49338.4414, grad_fn=<AddBackward0>)\n",
      "tensor(48641.4141, grad_fn=<AddBackward0>)\n",
      "tensor(48813.5195, grad_fn=<AddBackward0>)\n",
      "tensor(48871.8711, grad_fn=<AddBackward0>)\n",
      "tensor(48617.8125, grad_fn=<AddBackward0>)\n",
      "tensor(48604.0703, grad_fn=<AddBackward0>)\n",
      "tensor(47306.0352, grad_fn=<AddBackward0>)\n",
      "tensor(47363.7930, grad_fn=<AddBackward0>)\n",
      "tensor(46704.1562, grad_fn=<AddBackward0>)\n",
      "tensor(46878.7266, grad_fn=<AddBackward0>)\n",
      "tensor(46827.5273, grad_fn=<AddBackward0>)\n",
      "tensor(46485.3906, grad_fn=<AddBackward0>)\n",
      "tensor(47349.2148, grad_fn=<AddBackward0>)\n",
      "tensor(46308.8789, grad_fn=<AddBackward0>)\n",
      "tensor(45920.5195, grad_fn=<AddBackward0>)\n",
      "tensor(45988.1055, grad_fn=<AddBackward0>)\n",
      "tensor(45857.0156, grad_fn=<AddBackward0>)\n",
      "tensor(45807.2891, grad_fn=<AddBackward0>)\n",
      "tensor(44681.6211, grad_fn=<AddBackward0>)\n",
      "tensor(44320.3008, grad_fn=<AddBackward0>)\n",
      "tensor(44798.3828, grad_fn=<AddBackward0>)\n",
      "tensor(44383.6484, grad_fn=<AddBackward0>)\n",
      "tensor(44560.4570, grad_fn=<AddBackward0>)\n",
      "tensor(44132.6719, grad_fn=<AddBackward0>)\n",
      "tensor(43920.2930, grad_fn=<AddBackward0>)\n",
      "tensor(43573.6172, grad_fn=<AddBackward0>)\n",
      "tensor(44244.5078, grad_fn=<AddBackward0>)\n",
      "tensor(43892.0469, grad_fn=<AddBackward0>)\n",
      "tensor(44098.2344, grad_fn=<AddBackward0>)\n",
      "tensor(43424.3164, grad_fn=<AddBackward0>)\n",
      "tensor(43534.9961, grad_fn=<AddBackward0>)\n",
      "tensor(42955.7227, grad_fn=<AddBackward0>)\n",
      "tensor(42378.0820, grad_fn=<AddBackward0>)\n",
      "tensor(42278.8398, grad_fn=<AddBackward0>)\n",
      "tensor(42958.8398, grad_fn=<AddBackward0>)\n",
      "tensor(42357.4141, grad_fn=<AddBackward0>)\n",
      "tensor(42359.7539, grad_fn=<AddBackward0>)\n",
      "tensor(42238.4141, grad_fn=<AddBackward0>)\n",
      "tensor(41719.8398, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    encoder, decoder = torch.load('./model_CVAE/conv_variational_autoencoder.pkl')\n",
    "    print(\"\\n----------model restored----------\\n\")\n",
    "except:\n",
    "    print(\"\\n----------model not restored----------\\n\")\n",
    "    pass\n",
    "\n",
    "for i in range(num_epoch):\n",
    "    for j, [image, label] in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        mu, log_var, reparam = encoder(image)\n",
    "        output = decoder(reparam)\n",
    "        \n",
    "        loss = loss_function(output, image, mu, log_var)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if j % 10 == 0:\n",
    "            torch.save([encoder, decoder], './model_CVAE/conv_variational_autoencoder.pkl')\n",
    "            print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_img = torch.squeeze(output.cpu().data)\n",
    "print(out_img.size())\n",
    "\n",
    "plt.imshow(out_img[0].numpy(), cmap='gray')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
