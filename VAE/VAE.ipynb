{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nparser = argparse.ArgumentParser(description='VAE MNIST Example')\\nparser.add_argument('--batch-size', type=int, default=128, metavar='N', help='input batch size for training (default : 128)')\\nparser.add_argument('--epochs', type=int, default=10, metavar='N', help='number of epochs to train (default : 10)')\\nparser.add_argument('--no-cuda', action='store_true', default=False, help='enables CUDA training')\\nparser.add_argument('--seed', type=int, default=1, metavar='S', help='random seed (default : 1)')\\nparser.add_argument('--log-interval', type=int, default=10, metavar='N', help='how many batches to wait before logging training status')\\nargs = parser.parse_args()\\nargs.cuda = not args.no_cuda and torch.cuda.is_available()\\n\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "parser = argparse.ArgumentParser(description='VAE MNIST Example')\n",
    "parser.add_argument('--batch-size', type=int, default=128, metavar='N', help='input batch size for training (default : 128)')\n",
    "parser.add_argument('--epochs', type=int, default=10, metavar='N', help='number of epochs to train (default : 10)')\n",
    "parser.add_argument('--no-cuda', action='store_true', default=False, help='enables CUDA training')\n",
    "parser.add_argument('--seed', type=int, default=1, metavar='S', help='random seed (default : 1)')\n",
    "parser.add_argument('--log-interval', type=int, default=10, metavar='N', help='how many batches to wait before logging training status')\n",
    "args = parser.parse_args()\n",
    "args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import easydict\n",
    "\n",
    "args = easydict.EasyDict({\n",
    "    \"batch_size\" : 128,\n",
    "    \"epochs\" : 10,\n",
    "    \"no_cuda\" : False,\n",
    "    \"seed\" : 1,\n",
    "    \"log_interval\" : 10\n",
    "})\n",
    "args.cuda = not args.no_cuda and torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(args.seed)\n",
    "device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if args.cuda else {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.1%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "113.5%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.4%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "180.4%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('./data', \n",
    "                   train=True,\n",
    "                   download=True,\n",
    "                   transform=transforms.ToTensor()),\n",
    "    batch_size = args.batch_size,\n",
    "    shuffle = True,\n",
    "    **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('./data',\n",
    "                   train=False,\n",
    "                   transform=transforms.ToTensor()),\n",
    "    batch_size=args.batch_size,\n",
    "    shuffle=True,\n",
    "    **kwargs)            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### def encode(self, x)\n",
    "784 차원 벡터 입력 벡터 -> 400 차원 히든 벡터 -> 액티베이션 함수 ( Relu )  \n",
    "가우시안 분포에 대한 평균(mean)과 분산(variance)을 20개 내보낸다  \n",
    "Mean은 self.fc21(h1)이며 linear 연산을 통한 output 이다  \n",
    "Variance의 경우 항상 0보다 크거나 같아야한다. Variance의 경우 항상 0보다 크거나 같아야하는데  \n",
    "linear 연산을 한 self.fc22(h1)의 경우 -값이 될수 있다. 따라서 variance가 아닌 log variance라고 본다\n",
    "  \n",
    "##### reparameterize(self, mu, logvar)\n",
    "mean과 log variance를 구했다면 reparameterization으로 latent vector z를 샘플링 할 수 있다  \n",
    "Noise로부터 eps를 구하고 eps와 std를 곱하고 mean을 더해준다\n",
    "  \n",
    "##### decode(self, z)\n",
    "z를 구하고나면 이 latent 값으로부터 decoder를 통해 data에 대한 베르누이 분포를 출력할 수 있다  \n",
    "베르누이 분포는 0에서 1 사이이므로 sigmoid 함수를 output layer의 activation 함수로 사용한다\n",
    "  \n",
    "##### forward\n",
    "x.view(-1, 784)는 이미지를 784 차원의 벡터로 만드는 부분이다\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(784, 400)\n",
    "        self.fc21 = nn.Linear(400, 20)\n",
    "        self.fc22 = nn.Linear(400, 20)\n",
    "        self.fc3 = nn.Linear(20, 400)\n",
    "        self.fc4 = nn.Linear(400, 784)\n",
    "        \n",
    "    def encode(self, x):\n",
    "        h1 = F.relu(self.fc1(x))\n",
    "        return self.fc21(h1), self.fc22(h1)\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + (eps * std)\n",
    "    \n",
    "    def decode(self, z):\n",
    "        h3 = F.relu(self.fc3(z))\n",
    "        return torch.sigmoid(self.fc4(h3))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x.view(-1, 784))\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VAE().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruction + KL Divergence Losses summed over all elements and batch\n",
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    BCE = F.binary_cross_entropy(recon_x, x.view(-1, 784), reduction='sum')\n",
    "    # 0.5 * sum( 1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    KLD = -0.5 * torch.sum( 1 + logvar - mu.pow(2) - logvar.exp() )\n",
    "    \n",
    "    return BCE + KLD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### train( epoch )\n",
    "model.train()으로 현재 학습할 것이라는 선언을 한다  \n",
    "train_loader로 mini_batch를 추출한다  \n",
    "model에 data를 입력으로 넣어 출력을 받는다  \n",
    "출력으로 loss function 값을 계산하고 loss.backward()로 back-propagation으로 각 parameter의 gradient 값을 구한다\n",
    "optimizer( Adam Optmizer )를 통해 parameter를 update 한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train( epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        loss = loss_function(recon_batch, data, mu, logvar)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset), \n",
    "                100. * batch_idx / len(train_loader),\n",
    "                loss.item() / len(data)))\n",
    "            \n",
    "    print('====> Epoch: {} Average Loss: {:.4f}'.format(\n",
    "            epoch, train_loss / len(train_loader.dataset)))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### test ( epoch ) - 학습 과정을 evaluate\n",
    "model.eval()으로 평가 중이라는 것을 선언 한다  \n",
    "test dataset에 대해 reconstruction을 출력한다  \n",
    "loss 함수 값을 출력해서 학습이 어떻게 진행되고 있는지 평가한다  \n",
    "평가 과정마다 생성된 하나의 sample을 저장한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test( epoch):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (data, _) in enumerate(test_loader):\n",
    "            data = data.to(device) \n",
    "            recon_batch, mu, logvar = model(data)\n",
    "            test_loss += loss_function(recon_batch, data, mu, logvar).item()\n",
    "            if i == 0:\n",
    "                n = min(data.size(0), 8)\n",
    "                comparison = torch.cat( [ data[:n], recon_batch.view(args.batch_size, 1, 28, 28)[:n] ] )\n",
    "                save_image(comparison.cpu(),\n",
    "                          'results/reconstruction_' + str(epoch) + '.png', nrow=n)\n",
    "    \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('====> Test set loss : {:.4f}'.format(test_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### main 루프\n",
    "epoch 마다 train을 한다  \n",
    "test를 한 다음에 64개의 sample 이미지를 생성한다  \n",
    "latent는 임의로 20개를 normal distribution에서 sampling 한다  \n",
    "sampling 된 latent variable을 Decoder에 통과시키면 Decoder는 이미지를 생성한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 113.550560\n",
      "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 121.000389\n",
      "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 113.446571\n",
      "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 111.896294\n",
      "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 116.446449\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 117.376038\n",
      "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 117.175819\n",
      "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 117.438538\n",
      "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 116.461899\n",
      "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 114.874748\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 117.418427\n",
      "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 114.607094\n",
      "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 115.450699\n",
      "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 112.441147\n",
      "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 109.755478\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 115.406456\n",
      "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 113.809425\n",
      "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 115.295120\n",
      "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 114.975616\n",
      "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 116.396393\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 114.538963\n",
      "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 116.206474\n",
      "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 114.665665\n",
      "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 113.962234\n",
      "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 114.533394\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 116.454842\n",
      "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 114.699646\n",
      "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 112.434555\n",
      "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 116.166496\n",
      "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 117.893845\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 112.308182\n",
      "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 114.634300\n",
      "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 114.882446\n",
      "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 110.885994\n",
      "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 117.251205\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 113.547356\n",
      "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 115.882721\n",
      "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 111.986748\n",
      "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 109.341820\n",
      "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 109.270988\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 117.073318\n",
      "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 112.484650\n",
      "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 112.615799\n",
      "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 113.607246\n",
      "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 112.315338\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 113.882339\n",
      "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 114.770187\n",
      "====> Epoch: 1 Average Loss: 114.4909\n",
      "====> Test set loss : 111.7891\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 109.654953\n",
      "Train Epoch: 2 [1280/60000 (2%)]\tLoss: 110.173729\n",
      "Train Epoch: 2 [2560/60000 (4%)]\tLoss: 118.261536\n",
      "Train Epoch: 2 [3840/60000 (6%)]\tLoss: 110.330528\n",
      "Train Epoch: 2 [5120/60000 (9%)]\tLoss: 110.258766\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 110.703766\n",
      "Train Epoch: 2 [7680/60000 (13%)]\tLoss: 114.801750\n",
      "Train Epoch: 2 [8960/60000 (15%)]\tLoss: 112.479919\n",
      "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 112.831970\n",
      "Train Epoch: 2 [11520/60000 (19%)]\tLoss: 112.047897\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 110.515244\n",
      "Train Epoch: 2 [14080/60000 (23%)]\tLoss: 110.773491\n",
      "Train Epoch: 2 [15360/60000 (26%)]\tLoss: 108.419655\n",
      "Train Epoch: 2 [16640/60000 (28%)]\tLoss: 111.580276\n",
      "Train Epoch: 2 [17920/60000 (30%)]\tLoss: 111.971313\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 115.922012\n",
      "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 114.695480\n",
      "Train Epoch: 2 [21760/60000 (36%)]\tLoss: 107.198723\n",
      "Train Epoch: 2 [23040/60000 (38%)]\tLoss: 113.101410\n",
      "Train Epoch: 2 [24320/60000 (41%)]\tLoss: 113.659058\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 114.267624\n",
      "Train Epoch: 2 [26880/60000 (45%)]\tLoss: 109.112984\n",
      "Train Epoch: 2 [28160/60000 (47%)]\tLoss: 111.631027\n",
      "Train Epoch: 2 [29440/60000 (49%)]\tLoss: 111.917580\n",
      "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 108.063156\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 115.564072\n",
      "Train Epoch: 2 [33280/60000 (55%)]\tLoss: 117.049202\n",
      "Train Epoch: 2 [34560/60000 (58%)]\tLoss: 112.263962\n",
      "Train Epoch: 2 [35840/60000 (60%)]\tLoss: 111.037277\n",
      "Train Epoch: 2 [37120/60000 (62%)]\tLoss: 110.248093\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 111.004478\n",
      "Train Epoch: 2 [39680/60000 (66%)]\tLoss: 107.681007\n",
      "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 108.425636\n",
      "Train Epoch: 2 [42240/60000 (70%)]\tLoss: 113.461235\n",
      "Train Epoch: 2 [43520/60000 (72%)]\tLoss: 109.589447\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 111.645477\n",
      "Train Epoch: 2 [46080/60000 (77%)]\tLoss: 111.517891\n",
      "Train Epoch: 2 [47360/60000 (79%)]\tLoss: 112.347977\n",
      "Train Epoch: 2 [48640/60000 (81%)]\tLoss: 112.200790\n",
      "Train Epoch: 2 [49920/60000 (83%)]\tLoss: 114.459457\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 113.269943\n",
      "Train Epoch: 2 [52480/60000 (87%)]\tLoss: 109.303757\n",
      "Train Epoch: 2 [53760/60000 (90%)]\tLoss: 114.605110\n",
      "Train Epoch: 2 [55040/60000 (92%)]\tLoss: 111.051659\n",
      "Train Epoch: 2 [56320/60000 (94%)]\tLoss: 114.357681\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 112.954147\n",
      "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 110.321396\n",
      "====> Epoch: 2 Average Loss: 111.4638\n",
      "====> Test set loss : 109.4725\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 113.033775\n",
      "Train Epoch: 3 [1280/60000 (2%)]\tLoss: 109.257240\n",
      "Train Epoch: 3 [2560/60000 (4%)]\tLoss: 107.454521\n",
      "Train Epoch: 3 [3840/60000 (6%)]\tLoss: 110.239380\n",
      "Train Epoch: 3 [5120/60000 (9%)]\tLoss: 107.314453\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 111.903877\n",
      "Train Epoch: 3 [7680/60000 (13%)]\tLoss: 109.253143\n",
      "Train Epoch: 3 [8960/60000 (15%)]\tLoss: 109.817307\n",
      "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 108.793602\n",
      "Train Epoch: 3 [11520/60000 (19%)]\tLoss: 108.662399\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 110.579224\n",
      "Train Epoch: 3 [14080/60000 (23%)]\tLoss: 113.396576\n",
      "Train Epoch: 3 [15360/60000 (26%)]\tLoss: 108.445496\n",
      "Train Epoch: 3 [16640/60000 (28%)]\tLoss: 111.816025\n",
      "Train Epoch: 3 [17920/60000 (30%)]\tLoss: 105.948410\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 108.984329\n",
      "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 110.329781\n",
      "Train Epoch: 3 [21760/60000 (36%)]\tLoss: 109.184792\n",
      "Train Epoch: 3 [23040/60000 (38%)]\tLoss: 109.494057\n",
      "Train Epoch: 3 [24320/60000 (41%)]\tLoss: 111.245865\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 110.005981\n",
      "Train Epoch: 3 [26880/60000 (45%)]\tLoss: 109.435867\n",
      "Train Epoch: 3 [28160/60000 (47%)]\tLoss: 110.484200\n",
      "Train Epoch: 3 [29440/60000 (49%)]\tLoss: 106.684441\n",
      "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 108.266747\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 107.467354\n",
      "Train Epoch: 3 [33280/60000 (55%)]\tLoss: 110.030891\n",
      "Train Epoch: 3 [34560/60000 (58%)]\tLoss: 112.135910\n",
      "Train Epoch: 3 [35840/60000 (60%)]\tLoss: 105.918404\n",
      "Train Epoch: 3 [37120/60000 (62%)]\tLoss: 110.313354\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 110.170273\n",
      "Train Epoch: 3 [39680/60000 (66%)]\tLoss: 110.232544\n",
      "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 107.360115\n",
      "Train Epoch: 3 [42240/60000 (70%)]\tLoss: 111.827385\n",
      "Train Epoch: 3 [43520/60000 (72%)]\tLoss: 109.385880\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 110.713257\n",
      "Train Epoch: 3 [46080/60000 (77%)]\tLoss: 110.573456\n",
      "Train Epoch: 3 [47360/60000 (79%)]\tLoss: 108.035828\n",
      "Train Epoch: 3 [48640/60000 (81%)]\tLoss: 111.073326\n",
      "Train Epoch: 3 [49920/60000 (83%)]\tLoss: 112.995071\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 110.888290\n",
      "Train Epoch: 3 [52480/60000 (87%)]\tLoss: 110.846840\n",
      "Train Epoch: 3 [53760/60000 (90%)]\tLoss: 108.860214\n",
      "Train Epoch: 3 [55040/60000 (92%)]\tLoss: 106.893097\n",
      "Train Epoch: 3 [56320/60000 (94%)]\tLoss: 111.875237\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 106.949638\n",
      "Train Epoch: 3 [58880/60000 (98%)]\tLoss: 107.542419\n",
      "====> Epoch: 3 Average Loss: 109.7196\n",
      "====> Test set loss : 108.2173\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 110.669426\n",
      "Train Epoch: 4 [1280/60000 (2%)]\tLoss: 110.311989\n",
      "Train Epoch: 4 [2560/60000 (4%)]\tLoss: 109.202492\n",
      "Train Epoch: 4 [3840/60000 (6%)]\tLoss: 109.935875\n",
      "Train Epoch: 4 [5120/60000 (9%)]\tLoss: 109.100937\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 103.404564\n",
      "Train Epoch: 4 [7680/60000 (13%)]\tLoss: 106.169258\n",
      "Train Epoch: 4 [8960/60000 (15%)]\tLoss: 108.639366\n",
      "Train Epoch: 4 [10240/60000 (17%)]\tLoss: 104.955421\n",
      "Train Epoch: 4 [11520/60000 (19%)]\tLoss: 112.824524\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 105.816986\n",
      "Train Epoch: 4 [14080/60000 (23%)]\tLoss: 112.000229\n",
      "Train Epoch: 4 [15360/60000 (26%)]\tLoss: 110.614258\n",
      "Train Epoch: 4 [16640/60000 (28%)]\tLoss: 109.332207\n",
      "Train Epoch: 4 [17920/60000 (30%)]\tLoss: 107.618416\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 110.904427\n",
      "Train Epoch: 4 [20480/60000 (34%)]\tLoss: 106.037834\n",
      "Train Epoch: 4 [21760/60000 (36%)]\tLoss: 107.815453\n",
      "Train Epoch: 4 [23040/60000 (38%)]\tLoss: 108.522926\n",
      "Train Epoch: 4 [24320/60000 (41%)]\tLoss: 110.657745\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 107.435081\n",
      "Train Epoch: 4 [26880/60000 (45%)]\tLoss: 103.405159\n",
      "Train Epoch: 4 [28160/60000 (47%)]\tLoss: 109.388390\n",
      "Train Epoch: 4 [29440/60000 (49%)]\tLoss: 112.427460\n",
      "Train Epoch: 4 [30720/60000 (51%)]\tLoss: 113.035248\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 109.087051\n",
      "Train Epoch: 4 [33280/60000 (55%)]\tLoss: 106.252113\n",
      "Train Epoch: 4 [34560/60000 (58%)]\tLoss: 110.124710\n",
      "Train Epoch: 4 [35840/60000 (60%)]\tLoss: 108.651176\n",
      "Train Epoch: 4 [37120/60000 (62%)]\tLoss: 109.620621\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 108.248100\n",
      "Train Epoch: 4 [39680/60000 (66%)]\tLoss: 110.768616\n",
      "Train Epoch: 4 [40960/60000 (68%)]\tLoss: 110.307678\n",
      "Train Epoch: 4 [42240/60000 (70%)]\tLoss: 112.663284\n",
      "Train Epoch: 4 [43520/60000 (72%)]\tLoss: 108.020668\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 109.824265\n",
      "Train Epoch: 4 [46080/60000 (77%)]\tLoss: 111.685982\n",
      "Train Epoch: 4 [47360/60000 (79%)]\tLoss: 105.290237\n",
      "Train Epoch: 4 [48640/60000 (81%)]\tLoss: 111.209862\n",
      "Train Epoch: 4 [49920/60000 (83%)]\tLoss: 104.758194\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 109.209732\n",
      "Train Epoch: 4 [52480/60000 (87%)]\tLoss: 101.656418\n",
      "Train Epoch: 4 [53760/60000 (90%)]\tLoss: 106.399895\n",
      "Train Epoch: 4 [55040/60000 (92%)]\tLoss: 107.971756\n",
      "Train Epoch: 4 [56320/60000 (94%)]\tLoss: 110.630119\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 108.184120\n",
      "Train Epoch: 4 [58880/60000 (98%)]\tLoss: 109.982147\n",
      "====> Epoch: 4 Average Loss: 108.5881\n",
      "====> Test set loss : 107.3034\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 105.419189\n",
      "Train Epoch: 5 [1280/60000 (2%)]\tLoss: 105.752243\n",
      "Train Epoch: 5 [2560/60000 (4%)]\tLoss: 107.985229\n",
      "Train Epoch: 5 [3840/60000 (6%)]\tLoss: 110.478287\n",
      "Train Epoch: 5 [5120/60000 (9%)]\tLoss: 108.795380\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 110.932343\n",
      "Train Epoch: 5 [7680/60000 (13%)]\tLoss: 107.250381\n",
      "Train Epoch: 5 [8960/60000 (15%)]\tLoss: 111.920036\n",
      "Train Epoch: 5 [10240/60000 (17%)]\tLoss: 109.973251\n",
      "Train Epoch: 5 [11520/60000 (19%)]\tLoss: 105.058434\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 111.123772\n",
      "Train Epoch: 5 [14080/60000 (23%)]\tLoss: 106.066452\n",
      "Train Epoch: 5 [15360/60000 (26%)]\tLoss: 105.776817\n",
      "Train Epoch: 5 [16640/60000 (28%)]\tLoss: 106.016350\n",
      "Train Epoch: 5 [17920/60000 (30%)]\tLoss: 104.741676\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 106.413559\n",
      "Train Epoch: 5 [20480/60000 (34%)]\tLoss: 107.940567\n",
      "Train Epoch: 5 [21760/60000 (36%)]\tLoss: 104.937119\n",
      "Train Epoch: 5 [23040/60000 (38%)]\tLoss: 105.812355\n",
      "Train Epoch: 5 [24320/60000 (41%)]\tLoss: 110.121048\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 108.261337\n",
      "Train Epoch: 5 [26880/60000 (45%)]\tLoss: 109.827408\n",
      "Train Epoch: 5 [28160/60000 (47%)]\tLoss: 105.773827\n",
      "Train Epoch: 5 [29440/60000 (49%)]\tLoss: 106.019981\n",
      "Train Epoch: 5 [30720/60000 (51%)]\tLoss: 103.041946\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 106.290855\n",
      "Train Epoch: 5 [33280/60000 (55%)]\tLoss: 108.936150\n",
      "Train Epoch: 5 [34560/60000 (58%)]\tLoss: 106.288132\n",
      "Train Epoch: 5 [35840/60000 (60%)]\tLoss: 107.062874\n",
      "Train Epoch: 5 [37120/60000 (62%)]\tLoss: 107.800896\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 112.730591\n",
      "Train Epoch: 5 [39680/60000 (66%)]\tLoss: 105.732651\n",
      "Train Epoch: 5 [40960/60000 (68%)]\tLoss: 107.296829\n",
      "Train Epoch: 5 [42240/60000 (70%)]\tLoss: 108.945557\n",
      "Train Epoch: 5 [43520/60000 (72%)]\tLoss: 105.858063\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 111.133286\n",
      "Train Epoch: 5 [46080/60000 (77%)]\tLoss: 106.556824\n",
      "Train Epoch: 5 [47360/60000 (79%)]\tLoss: 111.570602\n",
      "Train Epoch: 5 [48640/60000 (81%)]\tLoss: 107.548630\n",
      "Train Epoch: 5 [49920/60000 (83%)]\tLoss: 109.126465\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 109.649399\n",
      "Train Epoch: 5 [52480/60000 (87%)]\tLoss: 106.357605\n",
      "Train Epoch: 5 [53760/60000 (90%)]\tLoss: 110.197517\n",
      "Train Epoch: 5 [55040/60000 (92%)]\tLoss: 108.303795\n",
      "Train Epoch: 5 [56320/60000 (94%)]\tLoss: 104.323959\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 105.831123\n",
      "Train Epoch: 5 [58880/60000 (98%)]\tLoss: 106.549179\n",
      "====> Epoch: 5 Average Loss: 107.7394\n",
      "====> Test set loss : 106.7229\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 105.699677\n",
      "Train Epoch: 6 [1280/60000 (2%)]\tLoss: 105.542221\n",
      "Train Epoch: 6 [2560/60000 (4%)]\tLoss: 105.781342\n",
      "Train Epoch: 6 [3840/60000 (6%)]\tLoss: 108.634331\n",
      "Train Epoch: 6 [5120/60000 (9%)]\tLoss: 105.815376\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 107.354431\n",
      "Train Epoch: 6 [7680/60000 (13%)]\tLoss: 104.567627\n",
      "Train Epoch: 6 [8960/60000 (15%)]\tLoss: 106.327255\n",
      "Train Epoch: 6 [10240/60000 (17%)]\tLoss: 105.043396\n",
      "Train Epoch: 6 [11520/60000 (19%)]\tLoss: 108.627663\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 108.991348\n",
      "Train Epoch: 6 [14080/60000 (23%)]\tLoss: 105.245712\n",
      "Train Epoch: 6 [15360/60000 (26%)]\tLoss: 102.470230\n",
      "Train Epoch: 6 [16640/60000 (28%)]\tLoss: 104.599045\n",
      "Train Epoch: 6 [17920/60000 (30%)]\tLoss: 104.005005\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 104.929924\n",
      "Train Epoch: 6 [20480/60000 (34%)]\tLoss: 112.987793\n",
      "Train Epoch: 6 [21760/60000 (36%)]\tLoss: 107.511398\n",
      "Train Epoch: 6 [23040/60000 (38%)]\tLoss: 107.845299\n",
      "Train Epoch: 6 [24320/60000 (41%)]\tLoss: 102.744995\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 107.496696\n",
      "Train Epoch: 6 [26880/60000 (45%)]\tLoss: 105.088654\n",
      "Train Epoch: 6 [28160/60000 (47%)]\tLoss: 106.987030\n",
      "Train Epoch: 6 [29440/60000 (49%)]\tLoss: 106.122894\n",
      "Train Epoch: 6 [30720/60000 (51%)]\tLoss: 106.857162\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 107.801598\n",
      "Train Epoch: 6 [33280/60000 (55%)]\tLoss: 109.861473\n",
      "Train Epoch: 6 [34560/60000 (58%)]\tLoss: 105.567642\n",
      "Train Epoch: 6 [35840/60000 (60%)]\tLoss: 110.158813\n",
      "Train Epoch: 6 [37120/60000 (62%)]\tLoss: 106.119698\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 107.738876\n",
      "Train Epoch: 6 [39680/60000 (66%)]\tLoss: 109.732101\n",
      "Train Epoch: 6 [40960/60000 (68%)]\tLoss: 108.849564\n",
      "Train Epoch: 6 [42240/60000 (70%)]\tLoss: 108.118835\n",
      "Train Epoch: 6 [43520/60000 (72%)]\tLoss: 109.345146\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 111.960213\n",
      "Train Epoch: 6 [46080/60000 (77%)]\tLoss: 109.476479\n",
      "Train Epoch: 6 [47360/60000 (79%)]\tLoss: 105.508820\n",
      "Train Epoch: 6 [48640/60000 (81%)]\tLoss: 107.689194\n",
      "Train Epoch: 6 [49920/60000 (83%)]\tLoss: 107.942169\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 104.327301\n",
      "Train Epoch: 6 [52480/60000 (87%)]\tLoss: 102.602890\n",
      "Train Epoch: 6 [53760/60000 (90%)]\tLoss: 106.160263\n",
      "Train Epoch: 6 [55040/60000 (92%)]\tLoss: 108.687759\n",
      "Train Epoch: 6 [56320/60000 (94%)]\tLoss: 104.820724\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 103.599442\n",
      "Train Epoch: 6 [58880/60000 (98%)]\tLoss: 108.932114\n",
      "====> Epoch: 6 Average Loss: 107.0488\n",
      "====> Test set loss : 106.1067\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 108.433655\n",
      "Train Epoch: 7 [1280/60000 (2%)]\tLoss: 104.025307\n",
      "Train Epoch: 7 [2560/60000 (4%)]\tLoss: 107.021652\n",
      "Train Epoch: 7 [3840/60000 (6%)]\tLoss: 107.592293\n",
      "Train Epoch: 7 [5120/60000 (9%)]\tLoss: 104.543335\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 108.985825\n",
      "Train Epoch: 7 [7680/60000 (13%)]\tLoss: 108.779671\n",
      "Train Epoch: 7 [8960/60000 (15%)]\tLoss: 104.086739\n",
      "Train Epoch: 7 [10240/60000 (17%)]\tLoss: 108.461769\n",
      "Train Epoch: 7 [11520/60000 (19%)]\tLoss: 106.493011\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 106.880180\n",
      "Train Epoch: 7 [14080/60000 (23%)]\tLoss: 104.991310\n",
      "Train Epoch: 7 [15360/60000 (26%)]\tLoss: 108.061584\n",
      "Train Epoch: 7 [16640/60000 (28%)]\tLoss: 108.576942\n",
      "Train Epoch: 7 [17920/60000 (30%)]\tLoss: 105.661240\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 109.547791\n",
      "Train Epoch: 7 [20480/60000 (34%)]\tLoss: 104.619812\n",
      "Train Epoch: 7 [21760/60000 (36%)]\tLoss: 102.695847\n",
      "Train Epoch: 7 [23040/60000 (38%)]\tLoss: 104.448318\n",
      "Train Epoch: 7 [24320/60000 (41%)]\tLoss: 105.860825\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 106.584007\n",
      "Train Epoch: 7 [26880/60000 (45%)]\tLoss: 102.779243\n",
      "Train Epoch: 7 [28160/60000 (47%)]\tLoss: 102.820702\n",
      "Train Epoch: 7 [29440/60000 (49%)]\tLoss: 109.948853\n",
      "Train Epoch: 7 [30720/60000 (51%)]\tLoss: 112.081100\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 105.834396\n",
      "Train Epoch: 7 [33280/60000 (55%)]\tLoss: 105.027794\n",
      "Train Epoch: 7 [34560/60000 (58%)]\tLoss: 104.083832\n",
      "Train Epoch: 7 [35840/60000 (60%)]\tLoss: 107.662918\n",
      "Train Epoch: 7 [37120/60000 (62%)]\tLoss: 106.197739\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 107.500687\n",
      "Train Epoch: 7 [39680/60000 (66%)]\tLoss: 106.855804\n",
      "Train Epoch: 7 [40960/60000 (68%)]\tLoss: 107.687241\n",
      "Train Epoch: 7 [42240/60000 (70%)]\tLoss: 110.209824\n",
      "Train Epoch: 7 [43520/60000 (72%)]\tLoss: 103.307526\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 105.797913\n",
      "Train Epoch: 7 [46080/60000 (77%)]\tLoss: 104.754242\n",
      "Train Epoch: 7 [47360/60000 (79%)]\tLoss: 103.508247\n",
      "Train Epoch: 7 [48640/60000 (81%)]\tLoss: 109.190865\n",
      "Train Epoch: 7 [49920/60000 (83%)]\tLoss: 108.255127\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 105.379929\n",
      "Train Epoch: 7 [52480/60000 (87%)]\tLoss: 106.047951\n",
      "Train Epoch: 7 [53760/60000 (90%)]\tLoss: 100.763687\n",
      "Train Epoch: 7 [55040/60000 (92%)]\tLoss: 106.327560\n",
      "Train Epoch: 7 [56320/60000 (94%)]\tLoss: 105.999939\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 107.258781\n",
      "Train Epoch: 7 [58880/60000 (98%)]\tLoss: 106.951988\n",
      "====> Epoch: 7 Average Loss: 106.6178\n",
      "====> Test set loss : 105.8466\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 106.781685\n",
      "Train Epoch: 8 [1280/60000 (2%)]\tLoss: 106.704742\n",
      "Train Epoch: 8 [2560/60000 (4%)]\tLoss: 106.901733\n",
      "Train Epoch: 8 [3840/60000 (6%)]\tLoss: 107.079834\n",
      "Train Epoch: 8 [5120/60000 (9%)]\tLoss: 101.939789\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 103.596252\n",
      "Train Epoch: 8 [7680/60000 (13%)]\tLoss: 106.622337\n",
      "Train Epoch: 8 [8960/60000 (15%)]\tLoss: 110.311958\n",
      "Train Epoch: 8 [10240/60000 (17%)]\tLoss: 103.945374\n",
      "Train Epoch: 8 [11520/60000 (19%)]\tLoss: 106.893105\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 100.740097\n",
      "Train Epoch: 8 [14080/60000 (23%)]\tLoss: 104.819565\n",
      "Train Epoch: 8 [15360/60000 (26%)]\tLoss: 110.721619\n",
      "Train Epoch: 8 [16640/60000 (28%)]\tLoss: 106.206802\n",
      "Train Epoch: 8 [17920/60000 (30%)]\tLoss: 102.660606\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 107.451027\n",
      "Train Epoch: 8 [20480/60000 (34%)]\tLoss: 107.511169\n",
      "Train Epoch: 8 [21760/60000 (36%)]\tLoss: 109.711884\n",
      "Train Epoch: 8 [23040/60000 (38%)]\tLoss: 97.159210\n",
      "Train Epoch: 8 [24320/60000 (41%)]\tLoss: 105.300697\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 108.058456\n",
      "Train Epoch: 8 [26880/60000 (45%)]\tLoss: 104.547180\n",
      "Train Epoch: 8 [28160/60000 (47%)]\tLoss: 106.974304\n",
      "Train Epoch: 8 [29440/60000 (49%)]\tLoss: 106.858742\n",
      "Train Epoch: 8 [30720/60000 (51%)]\tLoss: 102.396820\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 102.306458\n",
      "Train Epoch: 8 [33280/60000 (55%)]\tLoss: 105.996628\n",
      "Train Epoch: 8 [34560/60000 (58%)]\tLoss: 107.109268\n",
      "Train Epoch: 8 [35840/60000 (60%)]\tLoss: 106.545219\n",
      "Train Epoch: 8 [37120/60000 (62%)]\tLoss: 104.158493\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 106.144196\n",
      "Train Epoch: 8 [39680/60000 (66%)]\tLoss: 105.872086\n",
      "Train Epoch: 8 [40960/60000 (68%)]\tLoss: 109.666969\n",
      "Train Epoch: 8 [42240/60000 (70%)]\tLoss: 106.315880\n",
      "Train Epoch: 8 [43520/60000 (72%)]\tLoss: 104.531288\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 106.330956\n",
      "Train Epoch: 8 [46080/60000 (77%)]\tLoss: 111.551033\n",
      "Train Epoch: 8 [47360/60000 (79%)]\tLoss: 99.951019\n",
      "Train Epoch: 8 [48640/60000 (81%)]\tLoss: 108.368622\n",
      "Train Epoch: 8 [49920/60000 (83%)]\tLoss: 106.051682\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 107.102013\n",
      "Train Epoch: 8 [52480/60000 (87%)]\tLoss: 103.898392\n",
      "Train Epoch: 8 [53760/60000 (90%)]\tLoss: 102.616211\n",
      "Train Epoch: 8 [55040/60000 (92%)]\tLoss: 105.752289\n",
      "Train Epoch: 8 [56320/60000 (94%)]\tLoss: 110.152718\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 107.807541\n",
      "Train Epoch: 8 [58880/60000 (98%)]\tLoss: 103.431503\n",
      "====> Epoch: 8 Average Loss: 106.1671\n",
      "====> Test set loss : 105.6512\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 105.814468\n",
      "Train Epoch: 9 [1280/60000 (2%)]\tLoss: 111.518089\n",
      "Train Epoch: 9 [2560/60000 (4%)]\tLoss: 105.510323\n",
      "Train Epoch: 9 [3840/60000 (6%)]\tLoss: 101.618546\n",
      "Train Epoch: 9 [5120/60000 (9%)]\tLoss: 101.723076\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 106.238632\n",
      "Train Epoch: 9 [7680/60000 (13%)]\tLoss: 109.085831\n",
      "Train Epoch: 9 [8960/60000 (15%)]\tLoss: 107.903839\n",
      "Train Epoch: 9 [10240/60000 (17%)]\tLoss: 103.690887\n",
      "Train Epoch: 9 [11520/60000 (19%)]\tLoss: 107.812897\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 106.666496\n",
      "Train Epoch: 9 [14080/60000 (23%)]\tLoss: 103.312759\n",
      "Train Epoch: 9 [15360/60000 (26%)]\tLoss: 105.365700\n",
      "Train Epoch: 9 [16640/60000 (28%)]\tLoss: 106.433083\n",
      "Train Epoch: 9 [17920/60000 (30%)]\tLoss: 105.984085\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 106.502548\n",
      "Train Epoch: 9 [20480/60000 (34%)]\tLoss: 104.685898\n",
      "Train Epoch: 9 [21760/60000 (36%)]\tLoss: 102.837051\n",
      "Train Epoch: 9 [23040/60000 (38%)]\tLoss: 106.493561\n",
      "Train Epoch: 9 [24320/60000 (41%)]\tLoss: 105.449867\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 106.483353\n",
      "Train Epoch: 9 [26880/60000 (45%)]\tLoss: 100.593323\n",
      "Train Epoch: 9 [28160/60000 (47%)]\tLoss: 106.816910\n",
      "Train Epoch: 9 [29440/60000 (49%)]\tLoss: 105.476799\n",
      "Train Epoch: 9 [30720/60000 (51%)]\tLoss: 108.110649\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 105.991264\n",
      "Train Epoch: 9 [33280/60000 (55%)]\tLoss: 103.650528\n",
      "Train Epoch: 9 [34560/60000 (58%)]\tLoss: 104.626068\n",
      "Train Epoch: 9 [35840/60000 (60%)]\tLoss: 101.282867\n",
      "Train Epoch: 9 [37120/60000 (62%)]\tLoss: 112.973083\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 103.015503\n",
      "Train Epoch: 9 [39680/60000 (66%)]\tLoss: 104.295448\n",
      "Train Epoch: 9 [40960/60000 (68%)]\tLoss: 106.627747\n",
      "Train Epoch: 9 [42240/60000 (70%)]\tLoss: 99.728233\n",
      "Train Epoch: 9 [43520/60000 (72%)]\tLoss: 106.834564\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 104.937012\n",
      "Train Epoch: 9 [46080/60000 (77%)]\tLoss: 105.228127\n",
      "Train Epoch: 9 [47360/60000 (79%)]\tLoss: 107.692032\n",
      "Train Epoch: 9 [48640/60000 (81%)]\tLoss: 105.809250\n",
      "Train Epoch: 9 [49920/60000 (83%)]\tLoss: 106.979980\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 109.013084\n",
      "Train Epoch: 9 [52480/60000 (87%)]\tLoss: 105.073647\n",
      "Train Epoch: 9 [53760/60000 (90%)]\tLoss: 105.302612\n",
      "Train Epoch: 9 [55040/60000 (92%)]\tLoss: 106.521408\n",
      "Train Epoch: 9 [56320/60000 (94%)]\tLoss: 102.860901\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 106.243027\n",
      "Train Epoch: 9 [58880/60000 (98%)]\tLoss: 106.472717\n",
      "====> Epoch: 9 Average Loss: 105.8369\n",
      "====> Test set loss : 105.1079\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 106.264427\n",
      "Train Epoch: 10 [1280/60000 (2%)]\tLoss: 104.276321\n",
      "Train Epoch: 10 [2560/60000 (4%)]\tLoss: 104.711655\n",
      "Train Epoch: 10 [3840/60000 (6%)]\tLoss: 104.714783\n",
      "Train Epoch: 10 [5120/60000 (9%)]\tLoss: 107.065208\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 104.113358\n",
      "Train Epoch: 10 [7680/60000 (13%)]\tLoss: 101.992203\n",
      "Train Epoch: 10 [8960/60000 (15%)]\tLoss: 105.227547\n",
      "Train Epoch: 10 [10240/60000 (17%)]\tLoss: 105.974472\n",
      "Train Epoch: 10 [11520/60000 (19%)]\tLoss: 107.171440\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 106.531013\n",
      "Train Epoch: 10 [14080/60000 (23%)]\tLoss: 108.546768\n",
      "Train Epoch: 10 [15360/60000 (26%)]\tLoss: 106.296394\n",
      "Train Epoch: 10 [16640/60000 (28%)]\tLoss: 105.823418\n",
      "Train Epoch: 10 [17920/60000 (30%)]\tLoss: 108.722397\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 102.805962\n",
      "Train Epoch: 10 [20480/60000 (34%)]\tLoss: 105.484619\n",
      "Train Epoch: 10 [21760/60000 (36%)]\tLoss: 105.387764\n",
      "Train Epoch: 10 [23040/60000 (38%)]\tLoss: 105.376205\n",
      "Train Epoch: 10 [24320/60000 (41%)]\tLoss: 107.252357\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 106.794510\n",
      "Train Epoch: 10 [26880/60000 (45%)]\tLoss: 103.963737\n",
      "Train Epoch: 10 [28160/60000 (47%)]\tLoss: 102.425720\n",
      "Train Epoch: 10 [29440/60000 (49%)]\tLoss: 103.971283\n",
      "Train Epoch: 10 [30720/60000 (51%)]\tLoss: 105.761337\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 105.288177\n",
      "Train Epoch: 10 [33280/60000 (55%)]\tLoss: 104.392860\n",
      "Train Epoch: 10 [34560/60000 (58%)]\tLoss: 107.924568\n",
      "Train Epoch: 10 [35840/60000 (60%)]\tLoss: 102.496483\n",
      "Train Epoch: 10 [37120/60000 (62%)]\tLoss: 104.715408\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 103.500221\n",
      "Train Epoch: 10 [39680/60000 (66%)]\tLoss: 106.373184\n",
      "Train Epoch: 10 [40960/60000 (68%)]\tLoss: 105.741325\n",
      "Train Epoch: 10 [42240/60000 (70%)]\tLoss: 104.066711\n",
      "Train Epoch: 10 [43520/60000 (72%)]\tLoss: 103.647308\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 101.430222\n",
      "Train Epoch: 10 [46080/60000 (77%)]\tLoss: 105.288445\n",
      "Train Epoch: 10 [47360/60000 (79%)]\tLoss: 103.892319\n",
      "Train Epoch: 10 [48640/60000 (81%)]\tLoss: 102.790604\n",
      "Train Epoch: 10 [49920/60000 (83%)]\tLoss: 105.997200\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 108.789764\n",
      "Train Epoch: 10 [52480/60000 (87%)]\tLoss: 101.502975\n",
      "Train Epoch: 10 [53760/60000 (90%)]\tLoss: 104.713058\n",
      "Train Epoch: 10 [55040/60000 (92%)]\tLoss: 107.498322\n",
      "Train Epoch: 10 [56320/60000 (94%)]\tLoss: 107.183472\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 100.449509\n",
      "Train Epoch: 10 [58880/60000 (98%)]\tLoss: 105.079468\n",
      "====> Epoch: 10 Average Loss: 105.5223\n",
      "====> Test set loss : 105.2993\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        train( epoch )\n",
    "        test( epoch )\n",
    "        with torch.no_grad():\n",
    "            sample = torch.randn(64, 20).to(device)\n",
    "            sample = model.decode(sample).cpu()\n",
    "            save_image(sample.view(64, 1, 28, 28), 'results/sample_' + str(epoch) + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
