{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.7.6 64-bit ('recsys': conda)",
   "display_name": "Python 3.7.6 64-bit ('recsys': conda)",
   "metadata": {
    "interpreter": {
     "hash": "3e3ae0fa4a725afc921f1dce64743fc1e87f342ba5748ca33788c7a9a65676b3"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "각 단어로 구성된 입력값은 임베딩된 벡터로 변형된다. 이후 각 벡터를 평균해서 하나의 벡터로 만든다.     \n",
    "이후에 하나의 은닉층을 거친 후 하나의 결괏값을 뽑는 구조다.     \n",
    "마지막으로 나온 결괏깞에 시그모이드 함수를 적용해 0과 1 사이의 값을 구한다.    "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import preprocessing\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = [\n",
    "    '너 오늘 이뻐 보인다',\n",
    "    '나는 오늘 기분이 더러워',\n",
    "    '끝내주는데, 좋은 일이 있나봐',\n",
    "    '나 좋은 일이 생겼어',\n",
    "    '아 오늘 진짜 짜증나',\n",
    "    '환상적인데, 정말 좋은거 같아'\n",
    "]\n",
    "labels = [[1], [0], [1], [1], [0], [1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(samples)\n",
    "sequences = tokenizer.texts_to_sequences(samples)\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[[4, 1, 5, 6],\n",
       " [7, 1, 8, 9],\n",
       " [10, 2, 3, 11],\n",
       " [12, 2, 3, 13],\n",
       " [14, 1, 15, 16],\n",
       " [17, 18, 19, 20]]"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'오늘': 1,\n",
       " '좋은': 2,\n",
       " '일이': 3,\n",
       " '너': 4,\n",
       " '이뻐': 5,\n",
       " '보인다': 6,\n",
       " '나는': 7,\n",
       " '기분이': 8,\n",
       " '더러워': 9,\n",
       " '끝내주는데': 10,\n",
       " '있나봐': 11,\n",
       " '나': 12,\n",
       " '생겼어': 13,\n",
       " '아': 14,\n",
       " '진짜': 15,\n",
       " '짜증나': 16,\n",
       " '환상적인데': 17,\n",
       " '정말': 18,\n",
       " '좋은거': 19,\n",
       " '같아': 20}"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 변수를 정의\n",
    "batch_size = 2\n",
    "num_epochs = 100\n",
    "vocab_size = len(word_index) + 1\n",
    "emb_size = 128\n",
    "hidden_dimension = 256\n",
    "output_dimension = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequential 객체를 생성한 후 각 층을 추가한다.\n",
    "model = tf.keras.Sequential()\n",
    "model.add(layers.Embedding(vocab_size, emb_size, input_length = 4)) # 입력값을 임베딩\n",
    "model.add(layers.Lambda(lambda x: tf.reduce_mean(x, axis = 1))) # 각 단어 벡터를 평균\n",
    "model.add(layers.Dense(hidden_dimension, activation='relu')) \n",
    "model.add(layers.Dense(output_dimension, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "            loss='binary_crossentropy',\n",
    "            metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(sequences, labels, epochs=num_epochs, batch_size=batch_size)"
   ]
  },
  {
   "source": [
    "Functional API, Subclassing 방법으로 동일한 모델을 구현해보고 학습"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = layers.Input(shape = (4, ))\n",
    "embed_output = layers.Embedding(vocab_size, emb_size)(inputs)\n",
    "pooled_output = tf.reduce_mean(embed_output, axis=1)\n",
    "hidden_layer = layers.Dense(hidden_dimension, activation='relu')(pooled_output)\n",
    "outputs = layers.Dense(output_dimension, activation='sigmoid')(hidden_layer)\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizer.Adam(0.001),\n",
    "            loss='binary_crossentropy',\n",
    "            metrircs=['accuracy'])\n",
    "model.fit(sequences, labels, epochs=num_epochs, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomModel(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embed_dimension, hidden_dimension, output_dimension):\n",
    "        super(CustomModel, self).__init__(name='my_model')\n",
    "        self.embedding = layers.Embedding(vocab_size, embed_dimension)\n",
    "        self.dense_layer = layers.Dense(hidden_dimension, activation='relu')\n",
    "        self.output_layer = layers.Dense(output_dimension, activation='sigmoid')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.embedding(inputs)\n",
    "        x = tf.reduce_mean(x, axis = 1)\n",
    "        x = self.dense_layer(x)\n",
    "        x = self.output_layer(x)\n",
    "\n",
    "        return x \n",
    "    \n",
    "model = CustomModel(vocab_size=vocab_size,\n",
    "                    embed_dimension=emb_size,\n",
    "                    hidden_dimension=hidden_dimension,\n",
    "                    output_dimension=output_dimension)\n",
    "model.compile(optimizer=tf.keras.Adam(0.001),\n",
    "            loss='binary_crossentropy',\n",
    "            metrics=['accuracy'])\n",
    "model.fit(sequences, labels, epochs=num_epochs, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}